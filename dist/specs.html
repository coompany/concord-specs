<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="acidghost">
  <title>Concord</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" href="../style.css">
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header>
<h1 class="title">Concord</h1>
<h1 class="subtitle">A Distributed Voting Protocol</h1>
<h2 class="author">acidghost</h2>
</header>
<h2 id="network-protocol">Network Protocol</h2>
<p>Based on <em>Kademlia</em> <span class="citation" data-cites="maymounkov2002kademlia">(Maymounkov and Mazieres 2002)</span> and its extension <em>S/Kademlia</em> <span class="citation" data-cites="baumgart2007s">(Baumgart and Mies 2007)</span>. Each node is identified by a <code>nodeId</code> which is computed using a crypto puzzle. The puzzle generates also a key pair that is used to sign and verify integrity of messages.</p>
<p>Kademlia builds a <em>structured overlay</em> network, so nodes connections are not built at random, but following the protocol's rules. Data (in a file-sharing application for example) and routing information are held in a <em>distributed hash table</em> (DHT). The word distributed means that global information are not held anywhere and any peer maintains a small portion of it; more specifically only the portion it is responsible for, again following the protocol's rules.</p>
<p>Kademlia uses four RPCs for node / value lookup, routing table maintenance and storing values in the DHT:</p>
<ul>
<li><code>ping</code>: usual ping / pong request to check liveness of nodes;</li>
<li><code>store</code>: instructs the target node to store a <code>(key, value)</code> pair;</li>
<li><code>find_node</code>: takes a <code>nodeId</code> as an argument and the recipient of the request returns a list of <span class="math inline">\(k\)</span> triples <code>(IPaddr, UDPport, nodeId)</code> of nodes closest to the queried <code>nodeId</code>;</li>
<li><code>find_value</code>: behaves like <code>find_node</code> with the exception that if the target node has previously received a <code>store</code> request for the target key, it will return the stored value directly.</li>
</ul>
<p>At the moment Concord implements only <code>ping</code> and <code>find_node</code> RPCs. This is because the other ones seem related to a file-sharing application. The future development of the voting protocol will tell us if and in what extent Concord will need to implement <code>store</code> and <code>find_value</code> procedures.</p>
<p>Nodes are stored in a DHT. The local DHT is organized in <em>k-buckets</em>. For each bit in the <code>nodeId</code>, a node maintains a k-bucket, that is a FIFO list of nodes sorted with a last-seen policy (least recently seen node at the head). Let be <span class="math inline">\(n\)</span> the number of bits in the <code>nodeId</code>. Each node maintains <span class="math inline">\(n\)</span> k-buckets and each of them contains nodes at distance between <span class="math inline">\(2^i\)</span> and <span class="math inline">\(2^{i+1}\)</span> <span class="math inline">\(\forall i \in \{0,...,n-1\}\)</span>.</p>
<p><img src="graphviz-images/66849d954179b2c6724e48a48c3d5a3b662e1799.png" alt="" include="graphs/kademlia.dot" prog="dot" /></p>
<p>In the above Figure, we show Kademlia's DHT for nodeId <code>1100</code> in a network where the ID space is in 4 bits. Each color corresponds to a different k-bucket's range. In each k-bucket node IDs can be present or not (in the Figure, all nodes are shown as present) and each node maintains locally a list of <span class="math inline">\(k\)</span> nodes for each bucket. Note also that each k-bucket is responsible for progressively twice the number of node IDs when moving away from the local node's ID, that is <span class="math inline">\(n_i = 2 n_{i-1} \forall i \in \{1,...,n-1\}\)</span> and <span class="math inline">\(n_0 = 1\)</span> where <span class="math inline">\(n_i\)</span> is the number of possible node IDs in bucket <span class="math inline">\(i\)</span>. By moving away from the current node ID, each k-bucket covers twice the number of node IDs.</p>
<h3 id="broadcasting">Broadcasting</h3>
<p>This term is used to mean a <em>one to all</em> communication originating at a single node. Broadcasting is for example used in BitTorrent to announce to other peers when a node starts downloading a file, in Concord it is used to announce a new poll. The problem of efficient broadcasting (that is, with a minimum number of messages) has been tackled in <span class="citation" data-cites="el2003efficient">(El-Ansary et al. 2003)</span>. The paper focuses on Chord's DHT and overlay structure, but gives great hindsights on the subject. <span class="citation" data-cites="czirkos2010enhancing">(Zoltdn Czirkos and Hosszu 2010)</span> and <span class="citation" data-cites="czirkos2013solution">(Zoltán Czirkos and Hosszú 2013)</span> expand on the same idea, with a focus instead on Kademlia. The broadcasting algorithm exploits the network structure so to optimize the number of messages exchanged. The basic idea is an application of the divide-et-impera strategy: each node at each round is responsible for a smaller subtree than nodes at the previous round.</p>
<p><img src="graphviz-images/e160fcb8a6452c33ef19d7a45a5997f6f1491615.png" alt="" include="graphs/kademlia_broadcast.dot" prog="dot" /></p>
<p>The Figure above shows an example execution of the broadcast algorithm with node <code>1100</code> as the initiator. It starts the procedure by sending the message to a randomly selected node for each k-bucket. Each of the contacted node is then responsible for the subtree referenced by the k-bucket it belongs to. Each message contains also the height of the tree the receiving node is responsible for. Recursively then each of those nodes forwards the message to a randomly selected node in each of its k-buckets among those that reference nodes at the height it is responsible for (contained in the message originally received). When forwarding the message, each node increases the height field in the message, in order to limit the responsibility of the receiving node, based on its distance from the sender.</p>
<p><strong>Broadcast Algorithm 1</strong> <br /> <em>Input: message text, height</em></p>
<div class="sourceCode"><pre class="sourceCode ruby"><code class="sourceCode ruby">(<span class="dv">0</span>..height<span class="dv">-1</span>).each <span class="kw">do</span> |i|
  <span class="kw">if</span> (buckets[i].size != <span class="dv">0</span>) <span class="kw">then</span>
    node = random node from buckets[i]
    send(node, message, i)
  <span class="kw">end</span>
<span class="kw">end</span></code></pre></div>
<p>The algorithm above implements the broadcast. An initiator node sets the height to <span class="math inline">\(n - 1\)</span>, the number of k-buckets, and the message text at will. Then starts sending the message to a randomly picked node from each k-bucket <span class="math inline">\(i\)</span>. The buckets are sorted in a way that <code>buckets[0]</code> can contain maximum one node and is the closest possible from itself (their node IDs differs by only one least significant bit), that is k-bucket indexed with <span class="math inline">\(i\)</span> contains nodes at a distance between <span class="math inline">\(2^i\)</span> and <span class="math inline">\(2^{i+1}\)</span>. When sending a message to a node in the <span class="math inline">\(i\)</span>-th bucket, the sender sets the height in the message to <span class="math inline">\(i\)</span>, so to limit the broadcasting scope of the receiver. The height in this sense, represent the tree height-level already covered by other nodes.</p>
<p>The problem with this algorithm is that some nodes are responsible of large subtrees. If one of these nodes fails to deliver the message, big subtrees wont receive it. Consider the first round of broadcasting, so messages are sent by the initiator to a node in each bucket. The node picked from the farthest bucket will be the only responsible of forwarding the message to other nodes in its subtree, which covers addressed at a distance between <span class="math inline">\(2^{n-1}\)</span> and <span class="math inline">\(2^n\)</span>, that is half the global tree. If that node fails to deliver the message (because of any of the possible failure types, i.e. crash, Byzantine), then, supposing a perfect binary tree, half of the nodes in the network wont receive the message.</p>
<p>In <span class="citation" data-cites="czirkos2013solution">(Zoltán Czirkos and Hosszú 2013)</span> the <em>reliability</em> of the broadcasting algorithm is defined as the ratio between nodes that receive the message and the total number of nodes in the network: <span class="math inline">\(m = \frac{N_r}{N}\)</span> where <span class="math inline">\(N\)</span> is the total number of nodes and <span class="math inline">\(N_r\)</span> is the number of nodes that receive the message. The authors set a probabilistic framework to evaluate the reliability starting from a measure of <em>successful delegate selection</em>, that is a message is successfully broadcasted in a subtree through the selection of a delegate like Algorithm 1. They define a delegate selection to be successful if the packet does not encounter any of three cases I) it is lost, II) the receiver is Byzantine or III) the entry in the routing table for the receiver is stale. The successful delegation probability is <span class="math inline">\(P = 1 - P_h\)</span> where <span class="math inline">\(P_h\)</span> denotes the probability of failure. The authors show that for a balanced tree of height <span class="math inline">\(b\)</span>, the reliability of Algorithm 1 is given by <span class="math inline">\(m = \left( \frac{1 + P}{2} \right)^b\)</span>.</p>
<p>To improve the reliability of the broadcasting algorithm, the authors of the same paper proposed an upgraded version of Algorithm 1 that uses <em>replication</em>. In Algorithm 2 the message is sent to more nodes in the same k-bucket, in this way there is not any more a single point of failure in a subtree, but the responsibility is replicated among a number <span class="math inline">\(k_b\)</span> of them. To maintain efficiency it is required that <span class="math inline">\(k_b \le k\)</span> in order to avoid sending <code>find_node</code> requests. Moreover because a node can now receive a message more than once, each message has to be tagged with a unique identifier.</p>
<p><strong>Broadcast Algorithm 2</strong> <br /> <em>Input: identifier, message text, height</em></p>
<div class="sourceCode"><pre class="sourceCode ruby"><code class="sourceCode ruby"><span class="kw">return</span> <span class="kw">if</span> seen_messages.includes? identifier
seen_messages = seen_messages + [identifier]
(<span class="dv">0</span>..height<span class="dv">-1</span>).each <span class="kw">do</span> |i|
  <span class="kw">if</span> buckets[i].size != <span class="dv">0</span> <span class="kw">then</span>
    nodes = <span class="dt">Kb</span> random nodes from buckets[i]
    nodes.each <span class="kw">do</span> |node|
      send(node, identifier, message, i)
    <span class="kw">end</span>
  <span class="kw">end</span>
<span class="kw">end</span></code></pre></div>
<p>By selecting more nodes from each k-bucket, the probability of failing to broadcast the message in a subtree will be <span class="math inline">\(P_h^{k_b}\)</span> where <span class="math inline">\(P_h\)</span> is the probability of a single message getting lost and of the broadcast failing in the subtree the receiving node was responsible for. For example in a network with a fairly high amount of packets loss, lets say <span class="math inline">\(P_h = 10\%\)</span>, selecting a replication factor of <span class="math inline">\(k_b = 2\)</span> will reduce the probability of losing a message to <span class="math inline">\(P_h^2 = 1\%\)</span>, thus the probability of successful delegation is increased from <span class="math inline">\(P = 90\%\)</span> to <span class="math inline">\(P = 99\%\)</span>. By substituting <span class="math inline">\(P_h^{k_b}\)</span> into <span class="math inline">\(P_h\)</span> in the reliability's formula and solve for <span class="math inline">\(k_b\)</span>, we obtain an expression of the replication factor as a function of the reliability <span class="math inline">\(m\)</span>, the tree height <span class="math inline">\(b\)</span> and the probability of packet loss <span class="math inline">\(P_h\)</span>: <span class="math display">\[k_b = \left\lceil \frac{\ln (2 (1 - \sqrt[b]{m}))}{\ln P_h} \right\rceil\]</span></p>
<h2 id="voting-protocol">Voting Protocol</h2>
<p>The goal of this protocol is to allow anyone to create a poll and collect opinions on it. Each node is itself a voter and everyone is free to participate in the voting process. Each voter is also responsible of verification of votes and votes counting.</p>
<h3 id="starting-a-poll">Starting a Poll</h3>
<p>Whenever someone wants to start a poll, he / she has to send a <code>start_poll</code> RPC. This procedure contains the poll's question and its hash, computed from the text and a nonce. This hash is then used to build the <strong>poll's Merkle tree</strong> (details to come in a later section). Each node that wants to take part in the poll, has to find a key pair for which the hash of the public key is numerically less then the <code>start_poll</code> <em>genesis</em> hash (the created with the aforementioned RPC). The process is similar to the <code>nodeId</code> generation in S/Kademlia and is used to prevent huge amounts of valid key pairs for a single poll. The <code>start_poll</code> message contains also a TTL (time-to-live) indicating the maximum time allowed to find a valid key pair for the poll. TTL and crypto puzzle difficulty are measures that control the ability of nodes to generate an high number of valid key pairs.</p>
<p><img src="plantuml-images/39b131e2d5b230832898068db3332fd2442e8965.png" alt="" /></p>
<p>The node that starts the poll is responsible of sending the <code>start_poll</code> RPC to other nodes. To do so it has to send the RPC to all nodes that it knows about and sends the same request to other nodes that discovers through recursively call <code>find_node</code> on new nodes that discovers. Another strategy / extension could be to start searching for nodes randomly by generating a random <code>nodeId</code> starting a recursive lookup for that node and sending <code>start_poll</code> requests along the way. Nodes that decide to participate in the poll sends an <code>ack_poll</code> back to the node from which came to know about the poll. This RPC does not contain nodes and means that the node is taking responsibility to forward the <code>start_poll</code> message to its neighbors (all nodes that it knows about or only <span class="math inline">\(k\)</span> closest neighbors are options). Nodes that do not want to participate in the poll send an <code>nack_poll</code> RPC containing its <span class="math inline">\(k\)</span> closest nodes.</p>
<p>After the TTL expires, each node participating in the poll has a key pair that can be used to vote the poll.</p>
<h2 id="references" class="unnumbered"> References</h2>
<div id="refs" class="references">
<div id="ref-baumgart2007s">
<p>Baumgart, Ingmar, and Sebastian Mies. 2007. “S/Kademlia: A Practicable Approach Towards Secure Key-Based Routing.” In <em>Parallel and Distributed Systems, 2007 International Conference on</em>, 2:1–8. IEEE.</p>
</div>
<div id="ref-czirkos2013solution">
<p>Czirkos, Zoltán, and Gábor Hosszú. 2013. “Solution for the Broadcasting in the Kademlia Peer-to-Peer Overlay.” <em>Computer Networks</em> 57 (8). Elsevier: 1853–62.</p>
</div>
<div id="ref-czirkos2010enhancing">
<p>Czirkos, Zoltdn, and Gdbor Hosszu. 2010. “Enhancing the Kademlia P 2 P Network.” <em>Periodica Polytechnica, Electrical Engineering</em> 54 (3-4). Budapest University of Technology and Economics: 87–92.</p>
</div>
<div id="ref-el2003efficient">
<p>El-Ansary, Sameh, Luc Onana Alima, Per Brand, and Seif Haridi. 2003. “Efficient Broadcast in Structured P2p Networks.” In <em>International Workshop on Peer-to-Peer Systems</em>, 304–14. Springer.</p>
</div>
<div id="ref-maymounkov2002kademlia">
<p>Maymounkov, Petar, and David Mazieres. 2002. “Kademlia: A Peer-to-Peer Information System Based on the Xor Metric.” In <em>International Workshop on Peer-to-Peer Systems</em>, 53–65. Springer.</p>
</div>
</div>
</body>
</html>
